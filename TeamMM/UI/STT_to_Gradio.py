# ì½”ë“œ ë¸”ë¡ 6ê°œë¡œ êµ¬ì„± 
# #ì„¤ì •ì—ì„œ ìì‹ ì˜ í™˜ê²½ì— ë§ê²Œ ì„¤ì • 

# ì½”ë“œ ë¸”ë¡ 1: ê¸°ë³¸ ì„¤ì • ë° Vision ëª¨ë¸
import os
import ast
import json
import time
import xml.etree.ElementTree as ET
from glob import glob
import torch
import pandas as pd
import numpy as np
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoModelForVision2Seq, AutoProcessor, pipeline
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import gradio as gr
import gc

# ì„¤ì •
OPENAI_API_KEY = ""  # ì‹¤ì œ API í‚¤ë¡œ êµì²´
DATA_FOLDER = ""
PRODUCT_CSV_PATH = "/product_labels_final.csv"
WHISPER_MODEL_NAME = "SungBeom/whisper-small-ko"
EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
VISION_MODEL_NAME = "kakaocorp/kanana-1.5-v-3b-instruct"
GPT_MODEL_NAME = "gpt-4o-mini"
SIMILARITY_THRESHOLD = 0.25

# ë©”ëª¨ë¦¬ ê´€ë¦¬
def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def load_vision_model():
    """Vision ëª¨ë¸ ë¡œë“œ"""
    print("Vision ëª¨ë¸ ë¡œë“œ ì¤‘...")
    cleanup_memory()
    
    try:
        model = AutoModelForVision2Seq.from_pretrained(
            VISION_MODEL_NAME,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        model.eval()
        processor = AutoProcessor.from_pretrained(VISION_MODEL_NAME, trust_remote_code=True)
        print("âœ… Vision ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return model, processor
    except Exception as e:
        print(f"âŒ Vision ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def generate_image_caption(image_path, model, processor):
    """ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±"""
    try:
        image = Image.open(image_path).convert("RGB")
        
        batch = [{
            "image": [image],
            "conv": [
                {"role": "system", "content": "The following is a conversation between a curious human and AI assistant."},
                {"role": "user", "content": "<image>"},
                {"role": "user", "content": "ì´ ì‚¬ì§„ì— ëŒ€í•´ ì „ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜."},
            ]
        }]
        
        inputs = processor.batch_encode_collate(
            batch,
            padding_side="left",
            add_generation_prompt=True,
            max_length=8192
        )
        
        model_device = next(model.parameters()).device
        model_dtype = next(model.parameters()).dtype
        
        processed_inputs = {}
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                if value.dtype.is_floating_point:
                    processed_inputs[key] = value.to(device=model_device, dtype=model_dtype)
                else:
                    processed_inputs[key] = value.to(device=model_device)
            else:
                processed_inputs[key] = value
        
        gen_kwargs = {
            "max_new_tokens": 1024,
            "temperature": 0,
            "top_p": 1.0,
            "num_beams": 1,
            "do_sample": False,
        }
        
        with torch.no_grad():
            gens = model.generate(**processed_inputs, **gen_kwargs)
        
        caption = processor.tokenizer.batch_decode(gens, skip_special_tokens=True)[0]
        cleanup_memory()
        return caption
        
    except Exception as e:
        print(f"ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
        cleanup_memory()
        return f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {os.path.basename(image_path)}"

def parse_xml_metadata(xml_path):
    """XML ë©”íƒ€ë°ì´í„° íŒŒì‹±"""
    metadata = {}
    if os.path.exists(xml_path):
        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()
            div_cd = root.find('div_cd')
            if div_cd is not None:
                for tag in ["item_cd", "item_no", "div_l", "div_m", "div_s", "div_n", "comp_nm", "img_prod_nm", "volume"]:
                    el = div_cd.find(tag)
                    if el is not None:
                        metadata[tag] = el.text
        except Exception as e:
            print(f"XML íŒŒì‹± ì˜¤ë¥˜: {e}")
    return metadata

print("ì½”ë“œ ë¸”ë¡ 1 ì™„ë£Œ!")


# ì½”ë“œ ë¸”ë¡ 2: OpenAI ê´€ë ¨ í•¨ìˆ˜

def init_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    return OpenAI(api_key=OPENAI_API_KEY)

def generate_product_label_with_gpt(image_caption, metadata, client):
    """GPTë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ë¼ë²¨ ìƒì„±"""
    prompt = f"""
ë‹¤ìŒì€ ìƒí’ˆ ë©”íƒ€ë°ì´í„°ì™€ ì´ë¯¸ì§€ ìº¡ì…˜ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ ìº¡ì…˜ì— ë‚˜ì˜¤ëŠ” ìƒ‰ìƒ, ëª¨ì–‘, í¬ê¸° ë“± ì‹œê°ì  ì •ë³´ê¹Œì§€ í¬í•¨í•´,
ìƒí’ˆì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ë¸Œëœë“œ, ìš©ëŸ‰, ìš©ë„, ë””ìì¸ ë“±ì„ ëª¨ë‘ ë°˜ì˜í•œ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ì¸ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

[ë©”íƒ€ë°ì´í„°]
{metadata}

[ì´ë¯¸ì§€ ìº¡ì…˜]
{image_caption}
"""
    try:
        response = client.chat.completions.create(
            model=GPT_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"GPT ë¼ë²¨ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"ë¼ë²¨ ìƒì„± ì‹¤íŒ¨: {metadata.get('img_prod_nm', 'Unknown')}"

def extract_entities_with_features(text, client):
    """ìŒì„±ì—ì„œ ìƒí’ˆ ì—”í‹°í‹° ì¶”ì¶œ"""
    instruction = f'''
ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ë§í•œ ë¬¸ì¥ì…ë‹ˆë‹¤.
ì´ ë¬¸ì¥ì—ì„œ ì¥ë°”êµ¬ë‹ˆì— ë‹´ì„ ìƒí’ˆëª…ê³¼ ì£¼ìš” íŠ¹ì§•(ìš©ëŸ‰, ìˆ˜ëŸ‰ ë“±)ì„ í•¨ê»˜ í¬í•¨í•´ì„œ,
ê° í•­ëª©ì„ 'ìƒí’ˆëª…(íŠ¹ì§•)' í˜•íƒœë¡œ ë½‘ì•„ì£¼ì„¸ìš”. ìš©ëŸ‰ì´ ì—†ìœ¼ë©´ ìƒëµ, ìˆ˜ëŸ‰ì´ ì—†ìœ¼ë©´ 1ê°œë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
ì—¬ëŸ¬ ê°œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ ë½‘ì•„ì£¼ì„¸ìš”.

ë¬¸ì¥: "{text}"

ì˜ˆì‹œ)
ì…ë ¥: ì˜¤ëšœê¸° ì¹´ë ˆ ë¶„ë§ í•˜ë‚˜ë‘, í–‡ë°˜ ì‘ì€ ê±° ë‘ ê°œ ë„£ì–´ì¤„ë˜?
ì¶œë ¥: ì˜¤ëšœê¸° ì¹´ë ˆ ë¶„ë§ (1ê°œ), í–‡ë°˜ (ì‘ì€ ê±°, 2ê°œ)
'''
    
    try:
        response = client.chat.completions.create(
            model=GPT_MODEL_NAME,
            messages=[{"role": "user", "content": instruction}],
            temperature=0,
            max_tokens=200
        )
        result_text = response.choices[0].message.content.strip()
        entities = [e.strip() for e in result_text.split(",") if e.strip()]
        return entities
    except Exception as e:
        print(f"ì—”í‹°í‹° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [text]

print("ì½”ë“œ ë¸”ë¡ 2 ì™„ë£Œ!")



# ì½”ë“œ ë¸”ë¡ 3: ì œí’ˆ ë¼ë²¨ ìƒì„±

def create_product_labels_batch(image_files, batch_size=3):
    """ë°°ì¹˜ë¡œ ìƒí’ˆ ë¼ë²¨ ë°ì´í„° ìƒì„±"""
    vision_model, vision_processor = load_vision_model()
    openai_client = init_openai_client()
    
    if vision_model is None:
        return []
    
    product_data = []
    print(f"ì´ {len(image_files)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
    
    for batch_start in range(0, len(image_files), batch_size):
        batch_end = min(batch_start + batch_size, len(image_files))
        batch_files = image_files[batch_start:batch_end]
        
        print(f"\n=== ë°°ì¹˜ {batch_start//batch_size + 1}/{(len(image_files)-1)//batch_size + 1} ===")
        
        for idx, image_path in enumerate(batch_files):
            base_name = os.path.basename(image_path).replace(".jpg", "")
            xml_path = os.path.join(DATA_FOLDER, f"{base_name}_meta.xml")
            
            print(f"[{batch_start + idx + 1}/{len(image_files)}] ì²˜ë¦¬ ì¤‘: {base_name}")
            
            try:
                image_caption = generate_image_caption(image_path, vision_model, vision_processor)
                metadata = parse_xml_metadata(xml_path)
                label_sentence = generate_product_label_with_gpt(image_caption, metadata, openai_client)
                
                product_data.append({
                    'base_name': base_name,
                    'image_file': image_path,
                    'image_caption': image_caption,
                    'metadata': json.dumps(metadata, ensure_ascii=False),
                    'label_sentence': label_sentence
                })
                
                print(f"    ì™„ë£Œ: {label_sentence[:50]}...")
                
            except Exception as e:
                print(f"    ì˜¤ë¥˜ ë°œìƒ ({base_name}): {e}")
                continue
        
        cleanup_memory()
    
    return product_data

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²˜ìŒ 5ê°œ ì´ë¯¸ì§€ë§Œ)
if os.path.exists(DATA_FOLDER) and OPENAI_API_KEY != "your-openai-api-key-here":
    image_files = glob(os.path.join(DATA_FOLDER, "*.jpg"))[:5]  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
    
    if image_files:
        print("ìƒí’ˆ ë¼ë²¨ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        product_data = create_product_labels_batch(image_files, batch_size=2)
        
        # ê²°ê³¼ í™•ì¸
        print(f"\nìƒì„±ëœ ë¼ë²¨ ë°ì´í„°: {len(product_data)}ê°œ")
        for data in product_data:
            print(f"- {data['base_name']}: {data['label_sentence']}")
        
        # CSVë¡œ ì €ì¥ (í…ŒìŠ¤íŠ¸)
        if product_data:
            df = pd.DataFrame(product_data)
            test_csv_path = "/content/product_labels_test.csv"
            df.to_csv(test_csv_path, index=False, encoding='utf-8')
            print(f"\ní…ŒìŠ¤íŠ¸ CSV ì €ì¥ ì™„ë£Œ: {test_csv_path}")
    else:
        print("í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    print("ë°ì´í„° í´ë”ê°€ ì—†ê±°ë‚˜ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

print("ì½”ë“œ ë¸”ë¡ 3 ì™„ë£Œ!")



# ì½”ë“œ ë¸”ë¡ 4: ìŒì„± ì¸ì‹ ëª¨ë¸

def load_whisper_model():
    """Whisper ëª¨ë¸ ë¡œë“œ"""
    print("Whisper ëª¨ë¸ ë¡œë“œ ì¤‘...")
    whisper_model = pipeline(
        "automatic-speech-recognition",
        model=WHISPER_MODEL_NAME,
        device=0 if torch.cuda.is_available() else -1
    )
    print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return whisper_model

def process_voice_input(audio_file, whisper_model, openai_client):
    """ìŒì„± íŒŒì¼ ì²˜ë¦¬"""
    if audio_file is None:
        return "ìŒì„±ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", []
    
    try:
        # STT
        result = whisper_model(audio_file)
        recognized_text = result["text"]
        
        # ì—”í‹°í‹° ì¶”ì¶œ
        entities = extract_entities_with_features(recognized_text, openai_client)
        
        return recognized_text, entities
    except Exception as e:
        return f"ìŒì„± ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", []

# í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ ì—”í‹°í‹° ì¶”ì¶œ
if OPENAI_API_KEY != "your-openai-api-key-here":
    print("ì—”í‹°í‹° ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
    openai_client = init_openai_client()
    
    test_sentences = [
        "ì˜¤ëšœê¸° ì¹´ë ˆ ë¶„ë§ í•˜ë‚˜ë‘ í–‡ë°˜ ì‘ì€ ê±° ë‘ ê°œ ë„£ì–´ì¤„ë˜?",
        "íŒŒë€ìƒ‰ í¬ì¹´ì¹©ì´ë‘ ë³´ë¼ìƒ‰ê¹” í¬í‚¤ í•˜ë‚˜ ë‹´ì•„ì¤˜",
        "ë°”ë‚˜ë‚˜ ìš°ìœ  í•˜ë‚˜ë‘ ì´ˆì½”ìš°ìœ  200ml ë‘ ê°œ ë‹´ì•„ì¤˜"
    ]
    
    for sentence in test_sentences:
        entities = extract_entities_with_features(sentence, openai_client)
        print(f"ì…ë ¥: {sentence}")
        print(f"ì¶”ì¶œ: {entities}")
        print()

print("ì½”ë“œ ë¸”ë¡ 4 ì™„ë£Œ!")



# ì½”ë“œ ë¸”ë¡ 5: ìœ ì‚¬ë„ ê²€ìƒ‰ ëª¨ë“ˆ

def load_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return embedding_model

def load_product_data():
    """ìƒí’ˆ ë°ì´í„° ë¡œë“œ"""
    test_csv_path = "/content/product_labels_test.csv"
    
    if os.path.exists(PRODUCT_CSV_PATH):
        print("ê¸°ì¡´ ìƒí’ˆ ë°ì´í„° ë¡œë“œ ì¤‘...")
        product_data = pd.read_csv(PRODUCT_CSV_PATH)
    elif os.path.exists(test_csv_path):
        print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©...")
        product_data = pd.read_csv(test_csv_path)
    else:
        print("ìƒí’ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ë©”íƒ€ë°ì´í„° íŒŒì‹± ë° í•„ë“œ ì¶”ì¶œ
    def parse_metadata(metadata_str):
        try:
            if pd.isna(metadata_str) or metadata_str == '{}':
                return {}
            return ast.literal_eval(metadata_str) if isinstance(metadata_str, str) else metadata_str
        except:
            return {}
    
    product_data['parsed_metadata'] = product_data['metadata'].apply(parse_metadata)
    
    def extract_field(row, field_name):
        try:
            return row['parsed_metadata'].get(field_name, '')
        except:
            return ''
    
    product_data['brand'] = product_data.apply(lambda x: extract_field(x, 'comp_nm'), axis=1)
    product_data['product_name'] = product_data.apply(lambda x: extract_field(x, 'img_prod_nm'), axis=1)
    product_data['category_medium'] = product_data.apply(lambda x: extract_field(x, 'div_m'), axis=1)
    product_data['category_small'] = product_data.apply(lambda x: extract_field(x, 'div_s'), axis=1)
    product_data['volume'] = product_data.apply(lambda x: extract_field(x, 'volume'), axis=1)
    
    print(f"ìƒí’ˆ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(product_data)}ê°œ")
    return product_data

def find_similar_products(entities, product_data, embedding_model):
    """ì—”í‹°í‹°ì™€ ìœ ì‚¬í•œ ìƒí’ˆ ê²€ìƒ‰ - ì—”í‹°í‹°ë‹¹ ìµœê³  ìœ ì‚¬ë„ 1ê°œì”©ë§Œ"""
    if not entities or product_data.empty:
        return "ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", []
    
    try:
        all_results = []
        
        # ìƒí’ˆ ë¼ë²¨ë“¤ ë¯¸ë¦¬ ì„ë² ë”© (íš¨ìœ¨ì„±)
        product_labels = product_data['label_sentence'].tolist()
        product_embeddings = embedding_model.encode(product_labels)
        
        for entity in entities:
            print(f"ê²€ìƒ‰ ì¤‘: {entity}")
            
            # ì—”í‹°í‹° ì„ë² ë”©
            entity_embedding = embedding_model.encode([entity])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(entity_embedding, product_embeddings)[0]
            
            # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ìƒí’ˆ 1ê°œë§Œ ì„ íƒ
            best_idx = np.argmax(similarities)
            best_similarity = similarities[best_idx]
            
            if best_similarity > SIMILARITY_THRESHOLD:
                row = product_data.iloc[best_idx]
                all_results.append({
                    'query_entity': entity,
                    'matched_product': row['label_sentence'],
                    'brand': row.get('brand', 'N/A'),
                    'category': f"{row.get('category_medium', '')} > {row.get('category_small', '')}".strip(' > '),
                    'volume': row.get('volume', 'N/A'),
                    'similarity_score': float(best_similarity),
                    'image_path': row['image_file'],
                    'base_name': row['base_name']
                })
        
        if not all_results:
            return "ìœ ì‚¬í•œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result_text = f"ì°¾ì€ ìƒí’ˆ {len(all_results)}ê°œ:\n\n"
        images_to_show = []
        
        for i, result in enumerate(all_results):
            result_text += f"{i+1}. {result['matched_product']}\n"
            result_text += f"   ë§¤ì¹­: [{result['query_entity']}]\n"
            result_text += f"   ë¸Œëœë“œ: {result['brand']}\n"
            result_text += f"   ì¹´í…Œê³ ë¦¬: {result['category']}\n"
            result_text += f"   ìš©ëŸ‰: {result['volume']}\n"
            result_text += f"   ìœ ì‚¬ë„: {result['similarity_score']:.3f}\n\n"
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            try:
                if os.path.exists(result['image_path']):
                    img = Image.open(result['image_path']).convert('RGB')
                    images_to_show.append(img)
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return result_text, images_to_show
    
    except Exception as e:
        return f"ìƒí’ˆ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", []

# ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
if os.path.exists(PRODUCT_CSV_PATH) or os.path.exists("/content/product_labels_test.csv"):
    print("ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    
    # ëª¨ë¸ ë¡œë“œ
    embedding_model = load_embedding_model()
    product_data = load_product_data()
    
    if not product_data.empty:
        # í…ŒìŠ¤íŠ¸ ì—”í‹°í‹°ë“¤
        test_entities = [
            "ì˜¤ëšœê¸° ì¹´ë ˆ ë¶„ë§ (1ê°œ)",
            "íŒŒë€ìƒ‰ í¬ì¹´ì¹© (1ê°œ)",
            "ë°”ë‚˜ë‚˜ ìš°ìœ  (1ê°œ)"
        ]
        
        print(f"ìƒí’ˆ ë°ì´í„°: {len(product_data)}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ì—”í‹°í‹°: {test_entities}")
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        result_text, result_images = find_similar_products(test_entities, product_data, embedding_model)
        print("\n=== ê²€ìƒ‰ ê²°ê³¼ ===")
        print(result_text)
        print(f"ê²°ê³¼ ì´ë¯¸ì§€ ìˆ˜: {len(result_images)}")
    else:
        print("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
else:
    print("ìƒí’ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 3ë²ˆ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ìƒí’ˆ ë¼ë²¨ì„ ìƒì„±í•˜ì„¸ìš”.")

print("ì½”ë“œ ë¸”ë¡ 5 ì™„ë£Œ!")




# ì½”ë“œ ë¸”ë¡ 6: Gradio ì¸í„°í˜ì´ìŠ¤

def create_voice_shopping_app():
    """í†µí•© ìŒì„± ì‡¼í•‘ Gradio ì•±"""
    global whisper_model, embedding_model, product_data, openai_client
    
    print("ëª¨ë“  ëª¨ë¸ ë¡œë”© ì¤‘...")
    whisper_model = load_whisper_model()
    embedding_model = load_embedding_model()
    product_data = load_product_data()
    openai_client = init_openai_client()
    print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def process_voice_shopping(audio_file=None, microphone_audio=None):
        """ìŒì„± ì‡¼í•‘ í†µí•© ì²˜ë¦¬"""
        start_time = time.time()
        
        # ìŒì„± ì²˜ë¦¬
        if audio_file is not None:
            recognized_text, entities = process_voice_input(audio_file, whisper_model, openai_client)
        elif microphone_audio is not None:
            recognized_text, entities = process_voice_input(microphone_audio, whisper_model, openai_client)
        else:
            return "ìŒì„±ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "ì—”í‹°í‹°ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        if isinstance(recognized_text, str) and "ì˜¤ë¥˜" in recognized_text:
            return recognized_text, "ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨", []
        
        # ì—”í‹°í‹° ì •ë³´ í‘œì‹œ
        entities_text = "ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´:\n"
        for i, entity in enumerate(entities):
            entities_text += f"{i+1}. {entity}\n"
        
        # ìƒí’ˆ ê²€ìƒ‰
        product_results, product_images = find_similar_products(entities, product_data, embedding_model)
        
        processing_time = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼ ì¡°í•©
        final_text = f"ì¸ì‹ëœ ìŒì„±: {recognized_text}\n\n"
        final_text += f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
        final_text += product_results
        
        return final_text, entities_text, product_images
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
    with gr.Blocks(title="ìŒì„± ì‡¼í•‘ ì‹œìŠ¤í…œ", theme=gr.themes.Soft()) as app:
        
        gr.Markdown("""
        # ğŸ›’ AI ìŒì„± ì‡¼í•‘ ì‹œìŠ¤í…œ
        
        **ìŒì„±ìœ¼ë¡œ ìƒí’ˆì„ ì£¼ë¬¸í•˜ê³  AIê°€ ê´€ë ¨ ìƒí’ˆì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤!**
        
        ### ì‚¬ìš©ë²•
        1. ë§ˆì´í¬ë¡œ ìŒì„±ì„ ë…¹ìŒí•˜ê±°ë‚˜ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
        2. "ê²€ìƒ‰í•˜ê¸°" ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        3. AIê°€ ìŒì„±ì„ ì¸ì‹í•˜ê³  ê´€ë ¨ ìƒí’ˆì„ ì°¾ì•„ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤
        
        ### ë§í•˜ê¸° ì˜ˆì‹œ
        - "ì˜¤ëšœê¸° ì¹´ë ˆ ë¶„ë§ í•˜ë‚˜ë‘ ë†ì‹¬ ì‹ ë¼ë©´ ë‘ ê°œ"
        - "ì½”ì¹´ì½œë¼ 500ml í•˜ë‚˜ë‘ ìƒˆìš°ê¹¡ ë‹´ì•„ì¤˜"
        - "ë°”ë‚˜ë‚˜ ìš°ìœ  ì‘ì€ ê±¸ë¡œ í•˜ë‚˜, ì´ˆì½”íŒŒì´ í° ê±° ë‘ ê°œ"
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ¤ ìŒì„± ì…ë ¥")
                
                microphone_input = gr.Audio(
                    label="ë§ˆì´í¬ë¡œ ë…¹ìŒí•˜ê¸°",
                    type="filepath"
                )
                
                audio_file_input = gr.Audio(
                    label="ìŒì„± íŒŒì¼ ì—…ë¡œë“œ",
                    type="filepath"
                )
                
                with gr.Row():
                    search_btn = gr.Button("ğŸ” ê²€ìƒ‰í•˜ê¸°", variant="primary", size="lg")
                    clear_btn = gr.Button("ğŸ§¹ ì´ˆê¸°í™”", variant="secondary")
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
                
                result_text = gr.Textbox(
                    label="ìŒì„± ì¸ì‹ ë° ìƒí’ˆ ê²€ìƒ‰ ê²°ê³¼",
                    lines=12,
                    max_lines=20
                )
                
                entities_text = gr.Textbox(
                    label="ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´",
                    lines=6,
                    max_lines=10
                )
        
        gr.Markdown("### ğŸ›ï¸ ì°¾ì€ ìƒí’ˆë“¤")
        product_gallery = gr.Gallery(
            label="ê´€ë ¨ ìƒí’ˆ ì´ë¯¸ì§€",
            show_label=True,
            columns=3,
            rows=2,
            height=400,
            allow_preview=True
        )
        
        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        search_btn.click(
            fn=process_voice_shopping,
            inputs=[audio_file_input, microphone_input],
            outputs=[result_text, entities_text, product_gallery]
        )
        
        clear_btn.click(
            fn=lambda: (None, None, "", "", []),
            outputs=[audio_file_input, microphone_input, result_text, entities_text, product_gallery]
        )
    
    return app

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    # í•„ìˆ˜ ì„¤ì • ê²€ì¦
    if OPENAI_API_KEY == "your-openai-api-key-here":
        print("âŒ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
    elif not os.path.exists(DATA_FOLDER):
        print(f"âŒ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_FOLDER}")
    elif not (os.path.exists(PRODUCT_CSV_PATH) or os.path.exists("/content/product_labels_test.csv")):
        print("âŒ ìƒí’ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 3ë²ˆ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ë¼ë²¨ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    else:
        print("âœ… ìŒì„± ì‡¼í•‘ ì‹œìŠ¤í…œ ì‹œì‘!")
        app = create_voice_shopping_app()
        app.launch(
            share=True,
            server_name="0.0.0.0",
            server_port=7860,
            show_error=True
        )

# ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
def quick_test():
    """ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    test_text = "ë°”ë‚˜ë‚˜ ìš°ìœ  í•˜ë‚˜ë‘ ì´ˆì½”íŒŒì´ ë‘ ê°œ ë‹´ì•„ì¤˜"
    print(f"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: {test_text}")
    
    if OPENAI_API_KEY != "your-openai-api-key-here":
        client = init_openai_client()
        entities = extract_entities_with_features(test_text, client)
        print(f"ì¶”ì¶œëœ ì—”í‹°í‹°: {entities}")
        
        # ìƒí’ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if os.path.exists(PRODUCT_CSV_PATH) or os.path.exists("/content/product_labels_test.csv"):
            embedding_model = load_embedding_model()
            product_data = load_product_data()
            
            if not product_data.empty:
                result_text, images = find_similar_products(entities, product_data, embedding_model)
                print(f"ê²€ìƒ‰ ê²°ê³¼:\n{result_text[:200]}...")
                print(f"ê²°ê³¼ ì´ë¯¸ì§€ ìˆ˜: {len(images)}")
            else:
                print("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        else:
            print("ìƒí’ˆ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
# quick_test()

print("ì½”ë“œ ë¸”ë¡ 6 ì™„ë£Œ!")